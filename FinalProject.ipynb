{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "import gc\n",
    "RND_STATE = 100412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather links\n",
    "# ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTING_DATE = '2017-03-01'\n",
    "END_DATE = '2018-03-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'historical_data'\n",
    "DICT_FOLDER = 'dictionaries'\n",
    "WEATHER_FOLDER = 'weather_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'merged_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading flights data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(folder_name):\n",
    "    return glob.glob(folder_name + '/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    file_data = pd.read_csv(file_name, dtype={'CANCELLATION_CODE': str}, parse_dates=True)\n",
    "    file_data['FL_DATE'] = pd.to_datetime(file_data.FL_DATE)\n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_backup(file_name):\n",
    "    file_data = pd.read_csv(file_name, dtype={'cancellation_code': str}, parse_dates=True)\n",
    "    file_data['fl_date'] = pd.to_datetime(file_data.fl_date)\n",
    "    file_data = file_data.drop(['Unnamed: 0'], axis = 1)\n",
    "    return file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(folder_name):\n",
    "    file_list = get_file_list(folder_name)\n",
    "    \n",
    "    files_data = read_csv(file_list[0])\n",
    "    for file in log_progress(file_list[1:], every=1):\n",
    "        tmp = read_csv(file)\n",
    "        files_data = pd.concat([files_data, tmp])\n",
    "        del tmp\n",
    "        \n",
    "    files_data = files_data.reindex()\n",
    "    files_data.columns = map(str.lower, files_data.columns)\n",
    "    return files_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_city_names(files_data, dict_folder_name):\n",
    "    files_data_df = files_data.copy()\n",
    "    city_info = pd.read_csv(dict_folder_name + '/city_codes_info.csv')\n",
    "    city_info['Description'] =  city_info['Description'].str.replace(',.*|\\/.*| City', '')\n",
    "    \n",
    "    city_info.columns = ['origin_city_market_id', 'origin_city_name']\n",
    "    files_data_df = pd.merge(files_data_df, city_info, on='origin_city_market_id')\n",
    "    \n",
    "    city_info.columns = ['dest_city_market_id', 'dest_city_name']\n",
    "    files_data_df = pd.merge(files_data_df, city_info, on='dest_city_market_id')\n",
    "    \n",
    "    del city_info\n",
    "    \n",
    "    return files_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(folder_name, dict_folder_name):\n",
    "    files_data = gather_data(folder_name)\n",
    "    files_data = files_data.sort_values(by='fl_date')\n",
    "    files_data = files_data.dropna(thresh=9)\n",
    "    files_data = append_city_names(files_data, dict_folder_name)\n",
    "    return files_data.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(DATA_FOLDER, DICT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_city_names(data_df):\n",
    "    return list(set(list(data['origin_city_name'].values) + list(data['dest_city_name'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_city_codes(dict_folder_name):\n",
    "    tmp = pd.read_csv(dict_folder_name + '/weather_city_codes_info.csv')\n",
    "    tmp['name'] = tmp['name'].str.lower()\n",
    "    tmp['name'] = tmp['name'].str.replace(',.*|\\/.*| city', '')\n",
    "    tmp['name'] = tmp['name'].str.strip()\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_city_codes(data_df, dict_folder_name):\n",
    "    weather_cities_codes = load_city_codes(dict_folder_name)\n",
    "    data_city_names = get_data_city_names(data_df)\n",
    "    \n",
    "    processed_cities = []\n",
    "    failed_cities = []\n",
    "    for city in data_city_names:\n",
    "        city_data = weather_cities_codes[weather_cities_codes['name'].str.contains(city.lower())]\n",
    "        if len(city_data) == 0:\n",
    "            failed_cities.append(city)\n",
    "        else:\n",
    "            processed_cities.append({'name': city, 'weather_id': city_data['id'].values[0]})\n",
    "    return processed_cities, failed_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_for_city(city_info, start_date, end_date, api_key, limit=1000, offset=0):\n",
    "    req_url = 'http://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid={0}&startdate={1}&enddate={2}&limit={3}&offset={4}'\n",
    "    req_url = req_url.format(city_info['weather_id'], start_date, end_date, limit, offset)\n",
    "    \n",
    "    result_json = requests.get(req_url, headers={'token': api_key}, timeout=20)\n",
    "    result_json = json.loads(result_json.content)\n",
    "    result_data = pd.DataFrame(result_json['results'])\n",
    "    \n",
    "    if result_json['metadata']['resultset']['count'] > offset + limit:\n",
    "        return pd.concat([result_data, get_weather_for_city(city_info, start_date, end_date, api_key, limit, offset + limit)])\n",
    "    else:\n",
    "        return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weather_data(data_df, dict_folder_name, save_folder, start_date, end_date, api_key):\n",
    "    weather_city_codes, error_cities = get_weather_city_codes(data_df, dict_folder_name)\n",
    "    \n",
    "    for city in log_progress(weather_city_codes, every=1):\n",
    "        try:\n",
    "            city_weather = get_weather_for_city(city, start_date, end_date, api_key)\n",
    "            city_weather.to_csv(save_folder + '/' + city['name'] + '.csv')\n",
    "        except Exception as e:\n",
    "            error_cities.append(city)\n",
    "            print(e)\n",
    "    return weather_city_codes, error_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok, err = save_weather_data(data, DICT_FOLDER, WEATHER_FOLDER, STARTING_DATE, END_DATE, WEATHER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_available_weather(data_df, weather_data_folder):\n",
    "    city_list = get_file_list(weather_data_folder)\n",
    "    fixed_city_list = []\n",
    "    for city in city_list:\n",
    "        fixed_city_list.append(city.replace(weather_data_folder + '/', '').replace('.csv', ''))\n",
    "    \n",
    "    tmp_df = data_df[(data_df['origin_city_name'].isin(fixed_city_list)) | (data_df['dest_city_name'].isin(fixed_city_list))]\n",
    "    return tmp_df, city_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_weather_file(weather_data_folder, weather_file_path):\n",
    "    weather_data = pd.read_csv(weather_file_path)\n",
    "    weather_data['date'] = pd.to_datetime(weather_data.date)\n",
    "    weather_data = weather_data.drop(['Unnamed: 0', 'attributes', 'station'], axis=1)\n",
    "    weather_data = weather_data.rename(columns={'date': 'fl_date'})\n",
    "    weather_data = weather_data.drop_duplicates([\"fl_date\", \"datatype\"])\n",
    "    weather_data = weather_data.pivot_table(weather_data, index='fl_date', columns='datatype', aggfunc=sum, fill_value=0)\n",
    "    weather_data = weather_data.reset_index()\n",
    "    weather_data_value = weather_data['value'].copy()\n",
    "    weather_data_value['fl_date'] = weather_data['fl_date']\n",
    "    del weather_data\n",
    "    selected_cols = ['fl_date', 'AWND', 'SNOW', 'SNWD', 'WT01', 'WT02', 'WT03', 'WT04', 'WT05', 'WT06', 'WT07', 'WT08', 'WT09', 'WT10', 'WT11']\n",
    "    available_cols = list(set(weather_data_value.columns).intersection(selected_cols))\n",
    "    na_cols = list(set(selected_cols) - set(available_cols))\n",
    "    weather_data_value = weather_data_value[available_cols]\n",
    "    \n",
    "    for na_col in na_cols:\n",
    "        weather_data_value[na_col] = None\n",
    "    \n",
    "    weather_data_value = weather_data_value.rename({'SNOW': 'snowfall_mm', 'SNWD': 'snow_depth_mm', 'AWND': 'avg_wind_ms', 'WT08': 'smoke', 'WT01': 'fog', 'WT03': 'thunder'})\n",
    "    weather_data_value['city_name'] = weather_file_path.replace(weather_data_folder + '/', '').replace('.csv', '')\n",
    "    \n",
    "    return weather_data_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_with_weather(data_df, weather_data_folder):\n",
    "    filtered_df, city_list = filter_data_by_available_weather(data_df, weather_data_folder)\n",
    "    dfs = []\n",
    "    \n",
    "    for city in log_progress(city_list, every=1):\n",
    "        tmp = read_weather_file(weather_data_folder, city)\n",
    "        tmp = tmp.rename(columns={'city_name': 'origin_city_name'})\n",
    "        dfs.append(pd.merge(filtered_df, tmp, on=['fl_date', 'origin_city_name']))\n",
    "        tmp = tmp.rename(columns={'origin_city_name': 'dest_city_name'})\n",
    "        dfs.append(pd.merge(filtered_df, tmp, on=['fl_date', 'dest_city_name']))\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "        \n",
    "    final_df = pd.concat(dfs)\n",
    "    final_df = final_df.sort_values(by=['fl_date', 'origin_city_name', 'dest_city_name'])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3a7c5b1ad34785993c407253edf09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>VBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "VBox(children=(HTML(value=''), IntProgress(value=0, max=230)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = merge_data_with_weather(data, WEATHER_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv_backup(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_data):\n",
    "    data_df = file_data.copy()\n",
    "    data_df = data_df.drop(['origin_airport_id', 'origin_airport_seq_id', 'origin_city_market_id', 'dest_airport_id', 'dest_airport_seq_id', 'dest_city_market_id', 'year', 'quarter'], axis=1)\n",
    "    data_df = data_df.fillna(value={'cancellation_code': 'E'})\n",
    "    data_df = data_df.fillna(0)\n",
    "    data_df['fog'] = list(map(int , (data_df['WT01'] + data_df['WT02']).values > 0))\n",
    "    data_df['hail'] = list(map(int , (data_df['WT04'] + data_df['WT05']).values > 0))\n",
    "    data_df['damaging_wind'] = list(map(int , (data_df['WT10'] + data_df['WT11']).values > 0))\n",
    "    data_df = data_df.drop(['WT01', 'WT02', 'WT04', 'WT05', 'WT06', 'WT10', 'WT11'], axis=1)\n",
    "    data_df = data_df.rename(columns={'SNOW': 'snowfall', 'SNWD': 'snow_depth', 'AWND': 'average_wind_speed','WT03': 'thunder', 'WT07': 'dust', 'WT08': 'haze', 'WT09': 'snow'})\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional processing and value mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
